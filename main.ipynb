{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivvor2/RL-PEFT-a-small-reasoner/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 align=\"center\"></h3>\n",
        "\n",
        "<h1 align=\"center\">Qwen 0.5b on GRPO</h1>\n",
        "\n",
        "---\n",
        "\n",
        "<h1 align=\"center\">Training a small math reasoner with RL</h1>"
      ],
      "metadata": {
        "id": "Q7qTZbUcg5VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original notebook by [will brown,](https://x.com/willccbb), unfortunately, I can't find the X/Twitter release post anymore.\n",
        "\n",
        "On top of the original notebook, we have implemented:\n",
        "1. Evaluation code (to evaluate performance of the finetuned model vs the original model)\n",
        "2. LoRA finetuning (instead of full finetuning) of the model (in progress)\n",
        "\n",
        "Here is the release message for the original notebook\n",
        "\n",
        "> This notebook is an alternate version of the [GRPO demo](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) by [will brown,](https://x.com/willccbb) training llama-1b on the gsm8k math dataset.\n",
        "\n",
        "> We've only implemented a series of changes to make the code more workable on Colab:\n",
        "* Replacement of llama-1b with Qwen-0.5b\n",
        "* Generation with vllm, which yields a significant speed-up. Qwen small size makes it possible to run vllm on the same gpu as the one being used for GRPO.\n",
        "* Dropping flash-attn (recurrent bug with modeling qwen, not clear why)"
      ],
      "metadata": {
        "id": "gV4W0sp1UWKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the environment."
      ],
      "metadata": {
        "id": "GPYBrSbY79we"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install vllm. Notice that you'll have to restart the session afterwards."
      ],
      "metadata": {
        "id": "GOMhew_59RbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm"
      ],
      "metadata": {
        "id": "PYykgnUJ0BdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we install trl and datasets. It has to be in this order for some reason (bug on trl if you do vllm afterwards)"
      ],
      "metadata": {
        "id": "OJxgfykz93lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl datasets peft"
      ],
      "metadata": {
        "id": "ybtxR89X1YJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) We mount google drive for persistant storage.\n",
        "\n",
        "Change the root storage path if other forms of persistant storage is used"
      ],
      "metadata": {
        "id": "2NI_4aP7o8Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/ML_Experiments/qwen2.5_0.5B_GRPO_LoRA\"\n",
        "os.makedirs(os.path.dirname(base_path), exist_ok=True)"
      ],
      "metadata": {
        "id": "sCqAjH9FpFu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the RL rewards"
      ],
      "metadata": {
        "id": "ZJNTq5HG-EYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have everything ready to set up our RL training set and reward policy."
      ],
      "metadata": {
        "id": "Pbej_WBE6wLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we set the general prompt structure (with the reasoning tags)."
      ],
      "metadata": {
        "id": "bwqrjX1_-J3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# Load and prep dataset\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q04kVVaQ6dSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we import the gsm8k dataset and restructure it to fit into a conversational prompt format:"
      ],
      "metadata": {
        "id": "38AVgA19-PMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "# uncomment middle messages for 1-shot prompting\n",
        "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
        "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
        "    data = data.map(lambda x: { # type: ignore\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['question']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['answer'])\n",
        "    }) # type: ignore\n",
        "    return data # type: ignore\n",
        "\n",
        "dataset = get_gsm8k_questions()"
      ],
      "metadata": {
        "id": "fno7X8Fh-N6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We move on now to the reward functions. The most important one is the \"correctness\" function which acts as a verifier (comparison of model completions vs. answer). The three others are formatting functions."
      ],
      "metadata": {
        "id": "Yi-7Hs0T-YwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward functions\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]"
      ],
      "metadata": {
        "id": "BLCIyOzI0Gol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here are some additional helper functions to help find the latest checkpoint"
      ],
      "metadata": {
        "id": "Hor13Ut7aw0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def get_latest_checkpoint(base_dir: str):\n",
        "    \"\"\"Find the latest checkpoint in the given directory.\"\"\"\n",
        "\n",
        "    # Check existance for base directory\n",
        "    if not os.path.exists(base_dir):\n",
        "        print(f\"Warning: Directory {base_dir} does not exist\")\n",
        "        return None\n",
        "\n",
        "    # Look for checkpoint directories\n",
        "    checkpoint_dirs = [d for d in os.listdir(base_dir) if d.startswith('checkpoint-')]\n",
        "\n",
        "    if not checkpoint_dirs:\n",
        "        return None\n",
        "\n",
        "    # Extract checkpoint numbers and find the highest\n",
        "    checkpoint_nums = [int(re.search(r'checkpoint-(\\d+)', d).group(1)) for d in checkpoint_dirs]\n",
        "    latest_checkpoint_num = max(checkpoint_nums)\n",
        "    latest_checkpoint = f\"checkpoint-{latest_checkpoint_num}\"\n",
        "\n",
        "    return os.path.join(base_dir, latest_checkpoint)"
      ],
      "metadata": {
        "id": "u3YW095rawe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full finetuning and evaluation"
      ],
      "metadata": {
        "id": "rAOvX76rMpvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "cuz-LQOQ-vSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) Resume training from checkpoint"
      ],
      "metadata": {
        "id": "KPBb0Nu0hZu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now set the training arguments:"
      ],
      "metadata": {
        "id": "iGXAQetthWHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "output_dir=os.path.join(base_path, \"outputs/Qwen-0.5B-GRPO\")\n",
        "run_name=\"Qwen-0.5B-GRPO-gsm8k\"\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=output_dir,\n",
        "    run_name=run_name,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type='cosine',\n",
        "    logging_steps=1,\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_generations=16,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=100,\n",
        "    max_grad_norm=0.1,\n",
        "    log_on_each_node=False,\n",
        "    use_vllm=True,\n",
        "    vllm_gpu_memory_utilization=.3,\n",
        "    vllm_device=\"cuda:0\",\n",
        "    report_to=\"none\" #I'm disabling Wandb.\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "gGFFu5u4-3uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And launch the actual training:"
      ],
      "metadata": {
        "id": "REuVM0ep-4dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain checkpoint (to resume training)\n",
        "checkpoint_path = get_latest_checkpoint(output_dir)\n",
        "# checkpoint_path = None # Uncomment this if we want to restart training\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "if checkpoint_path is None: # No checkpoint\n",
        "    trainer.train()\n",
        "else:\n",
        "    trainer.train(resume_from_checkpoint=checkpoint_path) # resume training"
      ],
      "metadata": {
        "id": "U1ixGbPG0Ni-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the trained model"
      ],
      "metadata": {
        "id": "8CmMlmMkWWG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import SamplingParams, LLM"
      ],
      "metadata": {
        "id": "ZRMG4NZmZr1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = get_gsm8k_questions(split=\"test\")"
      ],
      "metadata": {
        "id": "7JdxrMuOWan9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load Trained Model & Tokenizer\n",
        "model_path = get_latest_checkpoint(output_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "1D4RZ64WXWQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Format Prompts Using Chat Template\n",
        "test_prompts = []\n",
        "for example in test_data:\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        example[\"prompt\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    test_prompts.append(formatted_prompt)"
      ],
      "metadata": {
        "id": "m_-b5QLpXaQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Set Up vLLM for Batch Inference\n",
        "llm = LLM(\n",
        "    model=model_path,\n",
        "    tensor_parallel_size=1,\n",
        "    gpu_memory_utilization=0.3,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 5. Configure Sampling Parameters\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.0,    # Greedy decoding for evaluation\n",
        "    max_tokens=200,     # Same as training's max_completion_length\n",
        "    stop=[\"<|im_end|>\"] # Qwen's stop token\n",
        ")"
      ],
      "metadata": {
        "id": "O8FsXx4eXhs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Generate Responses\n",
        "outputs = llm.generate(test_prompts, sampling_params)"
      ],
      "metadata": {
        "id": "gfP4ncCoXqol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Extract Answers\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    if \"<answer>\" in text and \"</answer>\" in text:\n",
        "        return text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
        "    return \"\"\n",
        "\n",
        "pred_answers = [extract_xml_answer(output.outputs[0].text) for output in outputs]\n",
        "true_answers = [example[\"answer\"] for example in test_data]"
      ],
      "metadata": {
        "id": "MMzyHMVLXsah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Calculate Accuracy\n",
        "accuracy = sum(1 for p, t in zip(pred_answers, true_answers) if p == t) / len(true_answers)\n",
        "print(f\"GSM8K Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "YrLMM31MXy1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) 9. Log the results\n",
        "results_path = os.path.join(base_path, \"/grpo_lora_results.txt\")\n",
        "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "\n",
        "with open(results_path, \"a\") as f:\n",
        "    f.write(f\"Baseline (full finetuning): {accuracy:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "h4W0vbi4tGsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA finetuning and evaluation"
      ],
      "metadata": {
        "id": "XqvOnXbyNKJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n"
      ],
      "metadata": {
        "id": "tAkwgSpZReZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first setup the PEFT (LoRA) configuration"
      ],
      "metadata": {
        "id": "zEi6vqh6jpfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "rank = 16\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=rank,                     # the rank of the loRA matrices\n",
        "    lora_alpha=2*rank,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",   # Attention layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "8u1FaoqMNNW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and setup the trainer like how it was previously (without VLLM as it does not support LoRA)"
      ],
      "metadata": {
        "id": "Y2NX5ROqRr-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "output_dir = os.path.join(base_path, f\"outputs/Qwen-0.5B-GRPO-LoRA-r{rank}\")\n",
        "run_name = f\"Qwen-0.5B-GRPO-LoRA-r{rank}-gsm8k\"\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=output_dir,\n",
        "    run_name=run_name,\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type='cosine',\n",
        "    logging_steps=1,\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=4, # Changed from 4 to 16 because otherwise the training would not start\n",
        "    num_generations=16,\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=200,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=100,\n",
        "    max_grad_norm=0.1,\n",
        "    log_on_each_node=False,\n",
        "    use_vllm=False,        # Use the PEFT model directly instead of vLLM engine\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "Agz8wf1pRix8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we launch the actural training"
      ],
      "metadata": {
        "id": "D-229LSbS7vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load checkpoint if it exists\n",
        "checkpoint_path = get_latest_checkpoint(output_dir)\n",
        "# checkpoint_path = None # Uncomment this if we want to restart training\n",
        "\n",
        "# Initialize GRPOTrainer with PEFT enabled\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config  # <-- Enables PEFT fine-tuning\n",
        ")\n",
        "\n",
        "# Start the training\n",
        "if checkpoint_path is None: # No checkpoint\n",
        "    trainer.train()\n",
        "else:\n",
        "    trainer.train(resume_from_checkpoint=checkpoint_path) # resume training"
      ],
      "metadata": {
        "id": "ghpWkUIaR8eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation with the PEFT model"
      ],
      "metadata": {
        "id": "c8boEzb1Wsz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We merge the trained LoRA adapter to our base model in order to evaluate using VLLM for better speed, as evaluation using `transformers` take over an hour\n",
        "\n",
        "The merged model should behave almost the same as the unmerged PEFT model (up to floating point rounding in matrix additions), so for the purpose of evaluation, the merged model should have the same performance as the unmerged model"
      ],
      "metadata": {
        "id": "vP_lg4fGavKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the model from the latest checkpoint and merge it (in memory)"
      ],
      "metadata": {
        "id": "3xV4xx_BigPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "import torch\n",
        "\n",
        "base_model = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
        "checkpoint_path = get_latest_checkpoint(output_dir)  # LoRA adapter\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "# If you saved tokenizer with PEFT, replace base_model with checkpoint_path above\n",
        "\n",
        "# Load the base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model, torch_dtype=torch.bfloat16, device_map=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Apply the LoRA adapter\n",
        "model = PeftModel.from_pretrained(model, checkpoint_path, is_trainable = False).to(\"cuda\")\n",
        "# model.eval()\n",
        "\n",
        "# Merging the model\n",
        "merged_model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "x6Js1SWyWsiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we merge and save the model to a temp directory, and then load the model with vLLM (which does not support reading from memory)"
      ],
      "metadata": {
        "id": "Q7VT0TYkhKdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "\n",
        "merged_dir = os.path.join(base_path, f\"merged/Qwen-0.5B-GRPO-LoRA-r{rank}\")\n",
        "os.makedirs(os.path.dirname(merged_dir), exist_ok=True)\n",
        "\n",
        "# Save merged model and tokenizer to the temp dir\n",
        "merged_model.save_pretrained(merged_dir)\n",
        "tokenizer.save_pretrained(merged_dir)"
      ],
      "metadata": {
        "id": "JQhnDUGthnN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And perform the evaluation on the gsm8k test\n",
        "\n",
        "(Restart the kernel before running the following code, as there will be errors when loading a model into `vllm` after it is loaded with `torch` or `transformers`, the following code is self-contained)"
      ],
      "metadata": {
        "id": "rWnDO8XlI10c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remount persistant storage if needed"
      ],
      "metadata": {
        "id": "kvP2rFnkwi0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/ML_Experiments/qwen2.5_0.5B_GRPO_LoRA\"\n",
        "\n",
        "rank = 16 # (Use the same rank you performred the test with)"
      ],
      "metadata": {
        "id": "1ac_s0ye0Dvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(os.path.dirname(base_path), exist_ok=True)"
      ],
      "metadata": {
        "id": "Fc98c3OYwiG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is identical to the full finetuning version (and changed to be self-contained)\n",
        "\n",
        "~~See, I know the whole reloading the kernel thing is awkward, but this project is done in a notebook and I don't want to deal with any subprocess shinenigans~~"
      ],
      "metadata": {
        "id": "EkF4WOAL1H79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import SamplingParams, LLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "base_model = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model) # The tokenizer is not updated during the PEFT finetuning process\n",
        "\n",
        "# 1. Load Test Data (if not already done)\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "def get_gsm8k_questions(split=\"test\"):\n",
        "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
        "    data = data.map(lambda x: {\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': x['question']}\n",
        "        ],\n",
        "        'answer': extract_hash_answer(x['answer'])\n",
        "    })\n",
        "    return data\n",
        "\n",
        "test_data = get_gsm8k_questions(split=\"test\")\n",
        "\n",
        "# 2. Format Prompts Using Chat Template\n",
        "test_prompts = []\n",
        "for example in test_data:\n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        example[\"prompt\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    test_prompts.append(formatted_prompt)\n",
        "\n",
        "# 3. Load the model into vLLM\n",
        "llm = LLM(\n",
        "model=os.path.join(base_path, f\"merged/Qwen-0.5B-GRPO-LoRA-r{rank}\"),  # Point to merged model path\n",
        "tensor_parallel_size=1,\n",
        "gpu_memory_utilization=0.3,\n",
        "trust_remote_code=True\n",
        ")\n",
        "\n",
        "# 4. Configure Sampling Parameters\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.0,\n",
        "    max_tokens=200,\n",
        "    stop=[\"<|im_end|>\"] # Qwen's stop token\n",
        ")\n",
        "\n",
        "# 5. Generate Responses\n",
        "outputs = llm.generate(test_prompts, sampling_params)\n",
        "\n",
        "# 6. Extract Answers\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    if \"<answer>\" in text and \"</answer>\" in text:\n",
        "        return text.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
        "    return \"\"\n",
        "\n",
        "pred_answers = [extract_xml_answer(output.outputs[0].text) for output in outputs]\n",
        "true_answers = [example[\"answer\"] for example in test_data]\n",
        "\n",
        "# 7. Calculate Accuracy\n",
        "accuracy = sum(p == t for p, t in zip(pred_answers, true_answers)) / len(true_answers)\n",
        "print(f\"GSM8K Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "kxQmcophXNtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_answers"
      ],
      "metadata": {
        "id": "7uCTIPLf2nis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "1hfGKyVG2gQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we log the results (optional)"
      ],
      "metadata": {
        "id": "r2YDmUEDjUeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_path = os.path.join(base_path, \"/grpo_lora_results.txt\")\n",
        "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
        "\n",
        "with open(results_path, \"a\") as f:\n",
        "    f.write(f\"Rank {rank}: {accuracy:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "sJLXKG2ujXPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete the model to perform another round of training"
      ],
      "metadata": {
        "id": "xUT8rVB8hzlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After downloading the checkpoint\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "X5LsbHvph3N6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}